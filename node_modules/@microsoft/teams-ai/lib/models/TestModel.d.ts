/**
 * @module teams-ai
 */
/**
 * Copyright (c) Microsoft Corporation. All rights reserved.
 * Licensed under the MIT License.
 */
import { PromptFunctions, PromptTemplate } from '../prompts';
import { PromptCompletionModel, PromptCompletionModelEmitter } from './PromptCompletionModel';
import { PromptResponse } from '../types';
import { Tokenizer } from '../tokenizers';
import { TurnContext } from 'botbuilder';
import { Memory } from '../MemoryFork';
/**
 * A `PromptCompletionModel` used for testing.
 */
export declare class TestModel implements PromptCompletionModel {
    private readonly _events;
    private readonly _handler;
    /**
     * Creates a new `OpenAIModel` instance.
     * @param {OpenAIModelOptions} options - Options for configuring the model client.
     * @param handler
     */
    constructor(handler: (model: TestModel, context: TurnContext, memory: Memory, functions: PromptFunctions, tokenizer: Tokenizer, template: PromptTemplate) => Promise<PromptResponse<string>>);
    /**
     * Events emitted by the model.
     * @returns {PromptCompletionModelEmitter} An event emitter for the model.
     */
    get events(): PromptCompletionModelEmitter;
    /**
     * Completes a prompt using OpenAI or Azure OpenAI.
     * @param {TurnContext} context - Current turn context.
     * @param {Memory} memory - An interface for accessing state values.
     * @param {PromptFunctions} functions - Functions to use when rendering the prompt.
     * @param {Tokenizer} tokenizer - Tokenizer to use when rendering the prompt.
     * @param {PromptTemplate} template - Prompt template to complete.
     * @returns {Promise<PromptResponse<string>>} A `PromptResponse` with the status and message.
     */
    completePrompt(context: TurnContext, memory: Memory, functions: PromptFunctions, tokenizer: Tokenizer, template: PromptTemplate): Promise<PromptResponse<string>>;
    static createTestModel(handler: (model: TestModel, context: TurnContext, memory: Memory, functions: PromptFunctions, tokenizer: Tokenizer, template: PromptTemplate) => Promise<PromptResponse<string>>): TestModel;
    static returnResponse(response: PromptResponse<string>, delay?: number): TestModel;
    static returnContent(content: string, delay?: number): TestModel;
    static returnError(error: Error, delay?: number): TestModel;
    static returnRateLimited(error: Error, delay?: number): TestModel;
    static streamTextChunks(chunks: string[], delay?: number): TestModel;
}
//# sourceMappingURL=TestModel.d.ts.map