"use strict";
/**
 * @module teams-ai
 */
/**
 * Copyright (c) Microsoft Corporation. All rights reserved.
 * Licensed under the MIT License.
 */
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.TestModel = void 0;
const events_1 = __importDefault(require("events"));
const StreamingResponse_1 = require("../StreamingResponse");
/**
 * A `PromptCompletionModel` used for testing.
 */
class TestModel {
    _events = new events_1.default();
    _handler;
    /**
     * Creates a new `OpenAIModel` instance.
     * @param {OpenAIModelOptions} options - Options for configuring the model client.
     * @param handler
     */
    constructor(handler) {
        this._handler = handler;
    }
    /**
     * Events emitted by the model.
     * @returns {PromptCompletionModelEmitter} An event emitter for the model.
     */
    get events() {
        return this._events;
    }
    /**
     * Completes a prompt using OpenAI or Azure OpenAI.
     * @param {TurnContext} context - Current turn context.
     * @param {Memory} memory - An interface for accessing state values.
     * @param {PromptFunctions} functions - Functions to use when rendering the prompt.
     * @param {Tokenizer} tokenizer - Tokenizer to use when rendering the prompt.
     * @param {PromptTemplate} template - Prompt template to complete.
     * @returns {Promise<PromptResponse<string>>} A `PromptResponse` with the status and message.
     */
    completePrompt(context, memory, functions, tokenizer, template) {
        return this._handler(this, context, memory, functions, tokenizer, template);
    }
    static createTestModel(handler) {
        return new TestModel(handler);
    }
    static returnResponse(response, delay = 0) {
        return new TestModel(async (model, context, memory, functions, tokenizer, template) => {
            model.events.emit('beforeCompletion', context, memory, functions, tokenizer, template, false);
            await new Promise((resolve) => setTimeout(resolve, delay));
            const streamer = new StreamingResponse_1.StreamingResponse(context);
            model.events.emit('responseReceived', context, memory, response, streamer);
            return response;
        });
    }
    static returnContent(content, delay = 0) {
        return TestModel.returnResponse({ status: 'success', message: { role: 'assistant', content } }, delay);
    }
    static returnError(error, delay = 0) {
        return TestModel.returnResponse({ status: 'error', error }, delay);
    }
    static returnRateLimited(error, delay = 0) {
        return TestModel.returnResponse({ status: 'rate_limited', error }, delay);
    }
    static streamTextChunks(chunks, delay = 0) {
        return new TestModel(async (model, context, memory, functions, tokenizer, template) => {
            model.events.emit('beforeCompletion', context, memory, functions, tokenizer, template, true);
            let content = '';
            for (let i = 0; i < chunks.length; i++) {
                await new Promise((resolve) => setTimeout(resolve, delay));
                const text = chunks[i];
                content += text;
                if (i === 0) {
                    model.events.emit('chunkReceived', context, memory, {
                        delta: { role: 'assistant', content: text }
                    });
                }
                else {
                    model.events.emit('chunkReceived', context, memory, { delta: { content: text } });
                }
            }
            // Finalize the response.
            await new Promise((resolve) => setTimeout(resolve, delay));
            const response = { status: 'success', message: { role: 'assistant', content } };
            const streamer = new StreamingResponse_1.StreamingResponse(context);
            model.events.emit('responseReceived', context, memory, response, streamer);
            return response;
        });
    }
}
exports.TestModel = TestModel;
//# sourceMappingURL=TestModel.js.map